{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89460da-c452-49bf-ab3a-9abe05513897",
   "metadata": {},
   "source": [
    "# Infra Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55f34ba-daa2-43fd-8dd7-74e10ca33771",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18e5d2f-78c0-40a7-88dd-2ee69bfd7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"looker-sandbox-323013\"\n",
    "REGION = \"europe-west4\"\n",
    "BUCKET_URI = f\"gs://vertex-ai-order-items-churn\"\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/order_items_churn\".format(BUCKET_URI)\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb220c41-ac85-435e-9298-01de2611ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shell_output = !gcloud auth list 2>/dev/null\n",
    "#SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "#print(SERVICE_ACCOUNT)\n",
    "\n",
    "SERVICE_ACCOUNT = \"1001913874856-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "#! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "#! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78afefe2-a489-40f3-aef4-0fec7ca0c71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: Incorrect public member type for binding aminehakkou@google.com:roles/storage.objectCreator\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch aminehakkou@google.com:roles/storage.objectCreator $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006df08-e808-416a-b6e2-bc9fc02dbdbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Components Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65027217-38e3-438c-b96c-e88a7cdc7731",
   "metadata": {},
   "source": [
    "## Artifacts type documentation : https://github.com/kubeflow/pipelines/blob/55a2fb5c20011b01945c9867ddff0d39e9db1964/sdk/python/kfp/v2/components/types/artifact_types.py#L255-L256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c1c87c-264a-443f-8c92-0eb67b087531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e389577-3e8b-4458-b938-74bcfd0dfe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp==1.8.14\n",
      "  Downloading kfp-1.8.14.tar.gz (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.3/304.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.8.14-py3-none-any.whl size=426472 sha256=2d5431651f858a114143f8ef17d7439557ad8e3eb9e51a9cae982d7a23a9597c\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/e7/b7/6d/b22f3f664269a163d3f0d15e01b723fa8695ca00f99983031e\n",
      "Successfully built kfp\n",
      "Installing collected packages: kfp\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 1.8.13\n",
      "    Uninstalling kfp-1.8.13:\n",
      "      Successfully uninstalled kfp-1.8.13\n",
      "Successfully installed kfp-1.8.14\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --no-deps kfp==1.8.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e524b72-2ab5-4458-b12d-c0444e5f6ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip freeze | grep kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4772d053-110f-406c-b3f7-71f64d647900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kfp.v2 import dsl\n",
    "from kfp import components\n",
    "\n",
    "from custom_components import (\n",
    "    automl_eval, \n",
    "    get_bq_job_output_table,\n",
    "    sklearn_trainer,\n",
    "    sklearn_validator,\n",
    "    generate_model_card,\n",
    "    export_from_bq_to_gcs,\n",
    "    tfdv\n",
    ")\n",
    "\n",
    "SQL_QUERY = open(\"sql/get_input_data.sql\", \"r\").read()\n",
    "\n",
    "#tfdv_generate_statistics = components.load_component_from_url(\"https://github.com/GoogleCloudPlatform/vertex-pipelines-end-to-end-samples/blob/main/pipelines/kfp_components/tfdv/generate_statistics.py\")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='churn',\n",
    "    description='A pipeline to detect churn from order_items transactions',\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline():\n",
    "    \n",
    "    #model_card = generate_model_card.generate_model_card(\n",
    "    #    gcp_project_id = PROJECT_ID\n",
    "    #)\n",
    "    \n",
    "    #dsl.ParallelFor([\"toto\", \"tata\"]) as item:\n",
    "    #    item.set_display_name(\"toto\")\n",
    "    #    model_card_loop = generate_model_card.generate_model_card(gcp_project_id = PROJECT_ID)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    from google_cloud_pipeline_components.v1.bigquery import BigqueryQueryJobOp\n",
    "    from google_cloud_pipeline_components.aiplatform import (\n",
    "        AutoMLTabularTrainingJobRunOp, \n",
    "        EndpointCreateOp, \n",
    "        ModelDeployOp,\n",
    "        TabularDatasetCreateOp\n",
    "    )\n",
    "    \n",
    "    bq_read = BigqueryQueryJobOp(\n",
    "        project = PROJECT_ID,\n",
    "        query = SQL_QUERY,\n",
    "        location = \"US\",\n",
    "        job_configuration_query = json.dumps(\n",
    "            {\n",
    "               \"destinationTable\": {\n",
    "                   \"projectId\": PROJECT_ID,\n",
    "                   \"datasetId\": \"churn_featuresets\",\n",
    "                   \"tableId\": \"churn_featureset_\" + TIMESTAMP\n",
    "               }\n",
    "            }\n",
    "        )\n",
    "    ).set_display_name(\"Generate Training Data\")\n",
    "    \n",
    "    gcs_export = export_from_bq_to_gcs.export_from_bq_to_gcs(\n",
    "        gcp_project_id = PROJECT_ID,\n",
    "        bq_job_output = bq_read.outputs['destination_table']\n",
    "    ).set_display_name(\"Export to GCS\")\n",
    "    \n",
    "    gen_statistics = tfdv.generate_statistics(\n",
    "        project_id = PROJECT_ID,\n",
    "        dataset = gcs_export.outputs[\"gcs_dataset\"],\n",
    "        file_pattern = \"*.csv\",\n",
    "        use_dataflow = False,\n",
    "        tfdv_stats_options = {\n",
    "            \"label_feature\" : \"is_churner\"\n",
    "        }\n",
    "\n",
    "    ).set_display_name(\"Generate data statistics\")\n",
    "    \n",
    "    #featureset = get_bq_job_output_table.get_bq_job_output_table(\n",
    "    #    bq_job_output = bq_read.outputs['destination_table']\n",
    "    #)\n",
    "    #tabular_dataset = TabularDatasetCreateOp(\n",
    "    #    display_name = \"churn_\" + TIMESTAMP,\n",
    "    #    bq_source = featureset.output,\n",
    "    #    project = PROJECT_ID\n",
    "    #)\n",
    "    #\n",
    "    #gen_statistics = generate_statistics(\n",
    "    #    dataset=tabular_dataset.outputs[\"dataset\"]\n",
    "    #).set_display_name(\"Generate data statistics\")\n",
    "    \n",
    "    #my_dict = my_list = ['foo', 'bar']\n",
    "    #with dsl.ParallelFor(my_dict) as item:\n",
    "    \n",
    "    sklearn_model = sklearn_trainer.sklearn_trainer(\n",
    "        input_table = bq_read.outputs['destination_table'],\n",
    "        gcp_project_id = PROJECT_ID\n",
    "    ).set_display_name(\"Model Training\")\n",
    "\n",
    "    sklearn_eval = sklearn_validator.sklearn_validator(        \n",
    "        input_model = sklearn_model.outputs[\"model\"],\n",
    "        input_table = bq_read.outputs['destination_table'],\n",
    "        gcp_project_id = PROJECT_ID,\n",
    "        thresholds_dict_str = '{\"roc\":0.8}'\n",
    "    ).set_display_name(\"Model Evaluation\")\n",
    "    \n",
    "    model_card = generate_model_card.generate_model_card(\n",
    "        gcp_project_id = PROJECT_ID ,\n",
    "        input_table = bq_read.outputs['destination_table'],\n",
    "        metrics = sklearn_eval.outputs['metrics']\n",
    "    ).set_display_name(\"Generate Model Card\")\n",
    "    \n",
    "    #featureset = get_bq_job_output_table.get_bq_job_output_table(\n",
    "    #    bq_job_output = bq_read.outputs['destination_table']\n",
    "    #)\n",
    "    #tabular_dataset = TabularDatasetCreateOp(\n",
    "    #    display_name = \"churn_\" + TIMESTAMP,\n",
    "    #    bq_source = featureset.output,\n",
    "    #    project = PROJECT_ID\n",
    "    #)\n",
    "    #automl_training = AutoMLTabularTrainingJobRunOp(\n",
    "    #    project = PROJECT_ID,\n",
    "    #    display_name = \"AutoML Training\",\n",
    "    #    model_display_name = \"automl_\" + TIMESTAMP,\n",
    "    #    \n",
    "    #    optimization_prediction_type=\"classification\",\n",
    "    #    optimization_objective=\"minimize-log-loss\",\n",
    "    #    \n",
    "    #    budget_milli_node_hours=1000,\n",
    "    #    disable_early_stopping = False,\n",
    "    #    \n",
    "    #    dataset = tabular_dataset.outputs[\"dataset\"],\n",
    "    #    #column_specs={\n",
    "    #    #    \"lifetime_orders\": \"numeric\",\n",
    "    #    #    \"nb_orders_last_12_days\": \"numeric\",\n",
    "    #    #    \"nb_orders_last_15_days\": \"numeric\",\n",
    "    #    #    \"nb_orders_last_7_days\": \"numeric\"\n",
    "    #    #},\n",
    "    #    target_column = \"is_churner\",\n",
    "    #    \n",
    "    #    predefined_split_column_name = \"split\"\n",
    "    #)\n",
    "    #model_eval_task = automl_eval.automl_classification_model_eval_metrics(\n",
    "    #    project = PROJECT_ID,\n",
    "    #    location = REGION,\n",
    "    #    thresholds_dict_str = '{\"auRoc\": 0.95}',\n",
    "    #    model = automl_training.outputs[\"model\"]\n",
    "    #)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a2eacf0-f652-4f37-a6d5-d5eec3fb1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"churn_pipeline.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663f843-16af-4630-849c-31de175b9d05",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "23119c12-1405-4b54-bde0-2be268625333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1001913874856/locations/us-central1/pipelineJobs/churn-20221120165658\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1001913874856/locations/us-central1/pipelineJobs/churn-20221120165658')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/churn-20221120165658?project=1001913874856\n",
      "PipelineJob projects/1001913874856/locations/us-central1/pipelineJobs/churn-20221120165658 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1001913874856/locations/us-central1/pipelineJobs/churn-20221120165658 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1001913874856/locations/us-central1/pipelineJobs/churn-20221120165658 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1001913874856/locations/us-central1/pipelineJobs/churn-20221120165658 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1001913874856/locations/us-central1/pipelineJobs/churn-20221120165658 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/1001913874856/locations/us-central1/pipelineJobs/churn-20221120165658\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "DISPLAY_NAME = \"churn_\" + TIMESTAMP\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"churn_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3e3b07b-533f-40cc-9164-763af1e31119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>bucket</th>\n",
       "      <th>lifetime_orders</th>\n",
       "      <th>nb_orders_last_7_days</th>\n",
       "      <th>nb_orders_last_15_days</th>\n",
       "      <th>nb_orders_last_12_days</th>\n",
       "      <th>is_churner</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72978</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72587</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72591</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72951</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72791</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  bucket  lifetime_orders  nb_orders_last_7_days  \\\n",
       "0    72978       2                3                      3   \n",
       "1    72587       4                3                      3   \n",
       "2    72591       4                3                      3   \n",
       "3    72951       5                3                      3   \n",
       "4    72791       6                3                      3   \n",
       "\n",
       "   nb_orders_last_15_days  nb_orders_last_12_days  is_churner  split  \n",
       "0                       3                       2       False  TRAIN  \n",
       "1                       3                       0       False  TRAIN  \n",
       "2                       3                       0       False  TRAIN  \n",
       "3                       3                       1       False  TRAIN  \n",
       "4                       3                       0       False  TRAIN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "sql = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `looker-sandbox-323013.churn_featuresets.churn_featureset_20220929092510`\n",
    "    where split != 'VALIDATE'\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bf82a54-c6f9-4357-813b-cf7d65552bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971,\n",
      "            ...\n",
      "            4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205],\n",
      "           dtype='int64', length=504)\n",
      "1962    False\n",
      "1963    False\n",
      "1964    False\n",
      "1965    False\n",
      "1966    False\n",
      "        ...  \n",
      "4201    False\n",
      "4202    False\n",
      "4203    False\n",
      "4204    False\n",
      "4205    False\n",
      "Name: is_churner, Length: 504, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# Identifying test indices\n",
    "print(df[df['split']=='TEST'].index)\n",
    "test_rows = df.iloc[df[df['split']=='TEST'].index]\n",
    "print(test_rows['is_churner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aaffc276-0b5a-4646-bc06-c90db044b460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "class columnDropperTransformer():\n",
    "    def __init__(self,columns):\n",
    "        self.columns=columns\n",
    "\n",
    "    def transform(self,X,y=None):\n",
    "        return X.drop(self.columns,axis=1)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "\n",
    "#train_rows = df.iloc[df[df['split']=='TRAIN'].index]\n",
    "#test_rows = df.iloc[df[df['split']=='TEST'].index]\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"columnDropper\", columnDropperTransformer(['user_id', 'bucket', 'split'])),\n",
    "        ('scaler', StandardScaler()),\n",
    "        #('feature_selection', SelectKBest(chi2, k=2)),\n",
    "        (\n",
    "            'classification', \n",
    "            RandomizedSearchCV(\n",
    "                RandomForestClassifier(),\n",
    "                param_distributions = {\n",
    "                    \"n_estimators\": np.arange(1, 2, 3),\n",
    "                    \"max_depth\": [3, 5]\n",
    "                },\n",
    "                n_iter = 2,\n",
    "                refit = True,\n",
    "                cv = PredefinedSplit(test_fold = df[df['split']=='TEST'].index) # Predefined split (no cross validation)\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "X, Y = df.drop(['is_churner'], axis=1), df['is_churner']\n",
    "\n",
    "pipeline.fit(X, Y)\n",
    "\n",
    "# Export the classifier to a file\n",
    "joblib.dump(pipeline, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c3745ab-c531-4b43-8514-cd5ff26783bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>bucket</th>\n",
       "      <th>lifetime_orders</th>\n",
       "      <th>nb_orders_last_7_days</th>\n",
       "      <th>nb_orders_last_15_days</th>\n",
       "      <th>nb_orders_last_12_days</th>\n",
       "      <th>is_churner</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50333</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>VALIDATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64795</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>VALIDATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31728</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>VALIDATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53224</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>VALIDATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49469</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>VALIDATE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  bucket  lifetime_orders  nb_orders_last_7_days  \\\n",
       "0    50333       7                1                      0   \n",
       "1    64795       7                1                      0   \n",
       "2    31728       7                1                      0   \n",
       "3    53224       7                1                      0   \n",
       "4    49469       7                1                      0   \n",
       "\n",
       "   nb_orders_last_15_days  nb_orders_last_12_days  is_churner     split  \n",
       "0                       1                       0       False  VALIDATE  \n",
       "1                       1                       0       False  VALIDATE  \n",
       "2                       1                       0       False  VALIDATE  \n",
       "3                       1                       0       False  VALIDATE  \n",
       "4                       1                       0       False  VALIDATE  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "sql = \"\"\"\n",
    "    SELECT *\n",
    "    FROM `looker-sandbox-323013.churn_featuresets.churn_featureset_20220929092510`\n",
    "    where split = 'VALIDATE'\n",
    "\"\"\"\n",
    "\n",
    "validation_df = client.query(sql).to_dataframe()\n",
    "\n",
    "validation_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75bf960a-a5b8-41be-8dc4-3a4220c2240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = joblib.load('model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3124ff8a-cf18-4444-84e0-abd443623a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92590751, 0.07409249],\n",
       "       [0.92590751, 0.07409249],\n",
       "       [0.92590751, 0.07409249],\n",
       "       ...,\n",
       "       [0.98461538, 0.01538462],\n",
       "       [0.98461538, 0.01538462],\n",
       "       [0.98461538, 0.01538462]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, Y_val = validation_df.drop(['is_churner'], axis=1), validation_df['is_churner']\n",
    "\n",
    "pipe.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d39a2fe2-0b25-45bb-843d-d62f24808db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_components import sklearn_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28482127-d80e-42ad-b9eb-b0e75f30f88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-9.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-9:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
